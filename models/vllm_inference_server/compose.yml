# docker-compose.yml file that defines a service for running a vLLM instance.
# It is designed to work in conjunction with a `.env` file for container's internal environment variables
# and a `vllm-config.yaml` file for server running-cli configuration.
# This approach provides a reusable and easily configurable setup for the vLLM server.

services:
  vllm-inference:
    container_name: vllm-inference-server
    image: vllm/vllm-openai:latest
    # Ensure the container has access to the GPU resources.
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Mount the host's Hugging Face cache to the container,
    # so the model is not re-downloaded every time.
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY}:${VLLM_CACHE_ROOT}/huggingface
      # Mount the configuration files to be used inside the container.
      # The vllm-config.yaml file contains CLI arguments.
      - ${CONFIG_FILE_DIRECTORY}/vllm-config.yaml:${VLLM_CONFIG_ROOT}/config.yaml
    # Expose the API server port.
    ports:
      - "${DOCKER_HOST_PORT}:8000"
    # Allow shared memory access, which is necessary for multi-processing.
    ipc: host
    # Load environment variables from the .env file.
    env_file:
      - .vllm.env

networks:
  default:
    name: vllm_network
