# docker-compose.yml file that defines a service for running a vLLM instance.
# It is designed to work in conjunction with a `.env` file for container's internal environment variables
# and a `vllm-config.yaml` file for server running-cli configuration.
# This approach provides a reusable and easily configurable setup for the vLLM server.

services:
  vllm-inference:
    container_name: vllm-inference-server
    image: vllm/vllm-openai:latest
    # Ensure the container has access to the GPU resources.
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      # This allows the container to access pre-downloaded models and datasets.
      - ${DOCKER_VOLUME_DIRECTORY}:${VLLM_CACHE_ROOT}

      # Mount the configuration files to be used inside the container.
      - ${CONFIG_FILE_DIRECTORY}/vllm-config.yaml:${VLLM_CONFIG_ROOT}/vllm-config.yaml
    ports:
      - "${DOCKER_HOST_PORT}:8000"
    # Allow shared memory access, which is necessary for multi-processing.
    ipc: host
    # Load environment variables from the .env file.
    env_file:
      - ./.vllm.env

    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8000/health"] # Adjust if vLLM exposes a different probe
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 40s

    # Override the default entrypoint to use 'vllm serve' and run with config file.
    command:
      - serve
      - --config
      - ${VLLM_CONFIG_ROOT}/vllm-config.yaml
