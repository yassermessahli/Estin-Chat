# docker-compose.yml file that defines a service for running a vLLM instance.
# It is designed to work in conjunction with a `.env` file for container's internal environment variables
# and a `vllm-config.yaml` file for server running-cli configuration.
# This approach provides a reusable and easily configurable setup for the vLLM server.

services:
  vllm-inference:
    container_name: vllm-inference-server
    image: vllm/vllm-openai:latest
    # Ensure the container has access to the GPU resources.
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Mount the host's Hugging Face cache to the container,
    # so the model is not re-downloaded every time.
    volumes:
      # This allows the container to access pre-downloaded models and datasets.
      - ${DOCKER_VOLUME_DIRECTORY}:${VLLM_CACHE_ROOT}/huggingface

      # Mount the configuration files to be used inside the container.
      # The vllm-config.yaml file contains CLI arguments.
      - ${CONFIG_FILE_DIRECTORY}/vllm-config.yaml:/vllm-config.yaml
    # Expose the API server port.
    ports:
      - "${DOCKER_HOST_PORT}:8000"
    # Allow shared memory access, which is necessary for multi-processing.
    ipc: host
    # Load environment variables from the .env file.
    env_file:
      - ./.vllm.env
    # Override the default entrypoint to use 'vllm serve' and run with config file.
    command:
      - serve
      - --config
      - /vllm-config.yaml
