# A Env file for configuring vLLM environment variables.
# This file can be used with Docker Compose or other tools
# to easily set the internal settings of a vLLM container.

# ====================================== Installation Time Env Vars ======================================

# Target device of vLLM, supporting [cuda (by default), rocm, neuron, cpu]
# Possible values: "cuda", "rocm", "neuron", "cpu"
VLLM_TARGET_DEVICE=cuda

# Maximum number of compilation jobs to run in parallel.
# By default this is the number of CPUs.
# MAX_JOBS=

# Number of threads to use for nvcc.
# By default this is 1. If set, `MAX_JOBS` will be reduced to avoid oversubscribing the CPU.
# NVCC_THREADS=

# If set, vllm will use precompiled binaries (*.so).
# Possible values: "1" or "0"
# VLLM_USE_PRECOMPILED=

# Whether to force using nightly wheel in python build. This is used for testing the nightly wheel in python build.
# Possible values: "1" or "0"
# VLLM_TEST_USE_PRECOMPILED_NIGHTLY_WHEEL=0

# CMake build type. If not set, defaults to "Debug" or "RelWithDebInfo".
# Available options: "Debug", "Release", "RelWithDebInfo"
# CMAKE_BUILD_TYPE=

# If set, vllm will print verbose logs during installation.
# Possible values: "1" or "0"
VERBOSE=1

# Root directory for vLLM configuration files.
# Defaults to `~/.config/vllm` unless `XDG_CONFIG_HOME` is set.
# Note that this not only affects how vllm finds its configuration files
# during runtime, but also affects how vllm installs its configuration
# files during **installation**.
VLLM_CONFIG_ROOT=/root/.config/vllm

# If set, vLLM will use the specified Hugging Face Hub token for authentication.
# This is useful for accessing private models or datasets.
# If you are using a private model or dataset, you should set this variable to your Hugging Face Hub token.
# Example: "hf_xxx"
# HUGGING_FACE_HUB_TOKEN=
# ============================================ Runtime Env Vars ==========================================

# Root directory for vLLM cache files.
# Defaults to `~/.cache/vllm` unless `XDG_CACHE_HOME` is set.
VLLM_CACHE_ROOT=/root/.cache/vllm

# Used in a distributed environment to determine the IP address of the current node, when the node has multiple network interfaces.
# If you are using multi-node inference, you should set this differently on each node.
# VLLM_HOST_IP=

# Used in a distributed environment to manually set the communication port.
# Note: if VLLM_PORT is set, and some code asks for multiple ports, the
# VLLM_PORT will be used as the first port, and the rest will be generated by incrementing the VLLM_PORT value.
# VLLM_PORT=

# Path used for IPC when the frontend API server is running in multi-processing mode to communicate with the backend engine process.
# VLLM_RPC_BASE_PATH=

# If true, will load models from ModelScope instead of Hugging Face Hub.
# Possible values: "true", "false"
VLLM_USE_MODELSCOPE=false

# Interval in seconds to log a warning message when the ring buffer is full.
# VLLM_RINGBUFFER_WARNING_INTERVAL=60

# Path to cudatoolkit home directory, under which should be bin, include, and lib directories.
# CUDA_HOME=

# Path to the NCCL library file. It is needed because nccl>=2.19 brought by PyTorch contains a bug: https://github.com/NVIDIA/nccl/issues/1234
# VLLM_NCCL_SO_PATH=

# When `VLLM_NCCL_SO_PATH` is not set, vllm will try to find the nccl library file in the locations specified by `LD_LIBRARY_PATH`.
# LD_LIBRARY_PATH=

# Flag to control if vllm should use triton flash attention.
# Possible values: "true", "1" (for true), "false", "0" (for false)
VLLM_USE_TRITON_FLASH_ATTN=1

# Use separate prefill and decode kernels for V1 attention instead of the unified triton kernel.
# Possible values: "true", "1" (for true), "false", "0" (for false)
VLLM_V1_USE_PREFILL_DECODE_ATTENTION=0

# Force vllm to use a specific flash-attention version (2 or 3), only valid when using the flash-attention backend.
# Possible values: "2", "3"
# VLLM_FLASH_ATTN_VERSION=

# Internal flag to enable Dynamo fullgraph capture.
# Possible values: "1" or "0"
VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE=1

# Feature flag to enable/disable Inductor standalone compile.
# In torch <= 2.7 this flag is ignored; in torch >= 2.8 this is enabled by default.
# Possible values: "1" or "0"
VLLM_USE_STANDALONE_COMPILE=1

# Local rank of the process in the distributed setting, used to determine the GPU device ID.
VLLM_LOCAL_RANK=0

# Used to control the visible devices in the distributed setting.
# Example: "0,1,2,3"
# CUDA_VISIBLE_DEVICES=

# Timeout for each iteration in the engine in seconds.
VLLM_ENGINE_ITERATION_TIMEOUT_S=60

# API key for vLLM API server.
VLLM_API_KEY=kdAP+2Eq9WDCPd+vPTr7460-gxWB3kxTs*2iglsSX4wW

# Whether to log responses from the API Server for debugging.
# Possible values: "true", "false"
VLLM_DEBUG_LOG_API_SERVER_RESPONSE=true

# S3 access information, used for tensorizer to load model from S3.
# S3_ACCESS_KEY_ID=
# S3_SECRET_ACCESS_KEY=
# S3_ENDPOINT_URL=

# Usage stats collection.
VLLM_USAGE_STATS_SERVER=https://stats.vllm.ai
VLLM_NO_USAGE_STATS=0
VLLM_DO_NOT_TRACK=0
VLLM_USAGE_SOURCE=production

# Logging configuration.
# If set to 0, vllm will not configure logging.
# If set to 1, vllm will configure logging using the default configuration or the configuration file specified by VLLM_LOGGING_CONFIG_PATH.
# Possible values: "1", "0"
VLLM_CONFIGURE_LOGGING=1

# Path to the logging configuration file.
# VLLM_LOGGING_CONFIG_PATH=

# This is used for configuring the default logging level.
# Possible values: "DEBUG" > "INFO" > "WARNING" > "ERROR" > "CRITICAL"
VLLM_LOGGING_LEVEL=INFO

# If set, VLLM_LOGGING_PREFIX will be prepended to all log messages.
VLLM_LOGGING_PREFIX="💠 "

# If set, vllm will call logits processors in a thread pool with this many threads.
# This is useful when using custom logits processors that either (a) launch additional CUDA kernels or (b) do significant CPU-bound work while not holding the python GIL, or both.
# Default is None.
# VLLM_LOGITS_PROCESSOR_THREADS=

# Trace function calls. If set to 1, vllm will trace function calls. Useful for debugging.
# Possible values: "1" or "0"
VLLM_TRACE_FUNCTION=0

# Backend for attention computation.
# Available options: "TORCH_SDPA", "FLASH_ATTN", "XFORMERS", "ROCM_FLASH", "FLASHINFER", "FLASHMLA"
# VLLM_ATTENTION_BACKEND=

# If set, vllm will use flashinfer sampler.
# Possible values: "1" or "0"
# VLLM_USE_FLASHINFER_SAMPLER=

# If set, vllm will force flashinfer to use tensor cores; otherwise will use heuristic based on model architecture.
# Possible values: "1" or "0"
# VLLM_FLASHINFER_FORCE_TENSOR_CORES=0

# Pipeline stage partition strategy.
# VLLM_PP_LAYER_PARTITION=

# (CPU backend only) CPU key-value cache space. Default is 4 GB.
# VLLM_CPU_KVCACHE_SPACE=

# (CPU backend only) CPU core IDs bound by OpenMP threads, e.g., "0-31", "0,1,2", "0-31,33". CPU cores of different ranks are separated by '|'.
# VLLM_CPU_OMP_THREADS_BIND=auto

# (CPU backend only) CPU cores not used by OMP threads. Those CPU cores will not be used by OMP threads of a rank.
# VLLM_CPU_NUM_OF_RESERVED_CPU=0

# (CPU backend only) whether to use prepack for MoE layer. On unsupported CPUs, you might need to set this to "0" (False).
# Possible values: "1" or "0"
# VLLM_CPU_MOE_PREPACK=1

# (CPU backend only) whether to use SGL kernels, optimized for small batch.
# Possible values: "1" or "0"
# VLLM_CPU_SGL_KERNEL=0

# If set, then all workers will execute as separate processes from the engine, and we use the same mechanism to trigger execution on all workers.
# Run vLLM with VLLM_USE_RAY_SPMD_WORKER=1 to enable it.
# Possible values: "1" or "0"
VLLM_USE_RAY_SPMD_WORKER=0

# If set, it uses the Ray's Compiled Graph (previously known as ADAG) API which optimizes the control plane overhead.
# Run vLLM with VLLM_USE_RAY_COMPILED_DAG=1 to enable it.
# Note that this variable is set to 1 in V1 by default when the ray distributed executor is used.
# Possible values: "1" or "0"
# VLLM_USE_RAY_COMPILED_DAG=0

# If set, Ray Compiled Graph uses the specified channel type to communicate between workers belonging to different pipeline-parallel stages.
# This flag is ignored if VLLM_USE_RAY_COMPILED_DAG is not set.
# Available options: "auto", "nccl", "shm"
# VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE=auto

# If set, it enables GPU communication overlap (experimental feature) in Ray's Compiled Graph.
# This flag is ignored if VLLM_USE_RAY_COMPILED_DAG is not set.
# Possible values: "1" or "0"
# VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM=0

# If set, it uses a Ray Communicator wrapping vLLM's pipeline parallelism communicator to interact with Ray's Compiled Graph.
# Otherwise, it uses Ray's NCCL communicator. This flag is ignored if VLLM_USE_RAY_COMPILED_DAG is not set.
# Possible values: "1" or "0"
# VLLM_USE_RAY_WRAPPED_PP_COMM=1

# Use dedicated multiprocess context for workers. Both spawn and fork work.
# Possible values: "fork", "spawn"
# VLLM_WORKER_MULTIPROC_METHOD=fork

# Path to the cache for storing downloaded assets.
# VLLM_ASSETS_CACHE=

# Timeout for fetching images when serving multimodal models in seconds.
# VLLM_IMAGE_FETCH_TIMEOUT=5

# Timeout for fetching videos when serving multimodal models in seconds.
# VLLM_VIDEO_FETCH_TIMEOUT=30

# Timeout for fetching audio when serving multimodal models in seconds.
# VLLM_AUDIO_FETCH_TIMEOUT=10

# Maximum filesize in MB for a single audio file when processing speech-to-text requests. Files larger than this will be rejected.
# VLLM_MAX_AUDIO_CLIP_FILESIZE_MB=25

# Backend for Video IO. Custom backend implementations can be registered via `@VIDEO_LOADER_REGISTRY.register("my_custom_video_loader")` and imported at runtime.
# Available options: "opencv"
# VLLM_VIDEO_LOADER_BACKEND=opencv

# Cache size (in GiB) for multimodal input cache.
# VLLM_MM_INPUT_CACHE_GIB=4

# Path to the XLA persistent cache directory. Only used for XLA devices such as TPUs.
# VLLM_XLA_CACHE_PATH=

# If set, assert on XLA recompilation after each execution step.
# Possible values: "1" or "0"
# VLLM_XLA_CHECK_RECOMPILATION=0

# Enable SPMD mode for TPU backend.
# Possible values: "1" or "0"
# VLLM_XLA_USE_SPMD=0

# VLLM_FUSED_MOE_CHUNK_SIZE=32768

# Control whether to use fused MoE activation chunking.
# Current chunking logic is incompatible with torch.compile and causes IMA. See issue https://github.com/vllm-project/vllm/issues/19631.
# Possible values: "1" or "0"
# VLLM_ENABLE_FUSED_MOE_ACTIVATION_CHUNKING=1

# If set, vllm will skip the deprecation warnings.
# Possible values: "1" or "0"
# VLLM_NO_DEPRECATION_WARNING=0

# If set, the OpenAI API server will stay alive even after the underlying AsyncLLMEngine errors and stops serving requests.
# Possible values: "1" or "0"
VLLM_KEEP_ALIVE_ON_ENGINE_DEATH=0

# If the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN is set, it allows the user to specify a max sequence length greater than the max length derived from the model's config.json.
# To enable this, set VLLM_ALLOW_LONG_MAX_MODEL_LEN=1.
# Possible values: "1" or "0"
VLLM_ALLOW_LONG_MAX_MODEL_LEN=0

# If set, forces FP8 Marlin to be used for FP8 quantization regardless of the hardware support for FP8 compute.
# Possible values: "1" or "0"
VLLM_TEST_FORCE_FP8_MARLIN=0

VLLM_TEST_FORCE_LOAD_FORMAT=dummy

# Time in ms for the zmq client to wait for a response from the backend server for simple data operations.
VLLM_RPC_TIMEOUT=10000

# Timeout in seconds for keeping HTTP connections alive in API server.
VLLM_HTTP_TIMEOUT_KEEP_ALIVE=5

# A list of plugin names to load, separated by commas.
# If this is not set, all plugins will be loaded.
# If this is set to an empty string, no plugins will be loaded.
# Example: "plugin1,plugin2"
# VLLM_PLUGINS=

# A local directory to look in for unrecognized LoRA adapters.
# Only works if plugins are enabled and VLLM_ALLOW_RUNTIME_LORA_UPDATING is enabled.
# VLLM_LORA_RESOLVER_CACHE_DIR=

# Enables torch profiler if set. Path to the directory where torch profiler traces are saved. Note that it must be an absolute path.
# VLLM_TORCH_PROFILER_DIR=

# Enable torch profiler to record shapes if set.
# Possible values: "1" or "0"
# VLLM_TORCH_PROFILER_RECORD_SHAPES=0

# Enable torch profiler to profile memory if set.
# Possible values: "1" or "0"
# VLLM_TORCH_PROFILER_WITH_PROFILE_MEMORY=0

# Enable torch profiler to profile stack if set. If not set, torch profiler WILL profile stack by default.
# Possible values: "1" or "0"
# VLLM_TORCH_PROFILER_WITH_STACK=1

# Enable torch profiler to profile flops if set.
# Possible values: "1" or "0"
# VLLM_TORCH_PROFILER_WITH_FLOPS=0

# If set, vLLM will use Triton implementations of AWQ.
# Possible values: "1" or "0"
# VLLM_USE_TRITON_AWQ=0

# If set, allow loading or unloading lora adapters in runtime.
# Possible values: "1" or "0"
# VLLM_ALLOW_RUNTIME_LORA_UPDATING=0

# Use the TensorRT-LLM decode attention backend in flashinfer.
# Possible values: "1" or "0"
# VLLM_USE_TRTLLM_DECODE_ATTENTION=0

# Controls garbage collection during CUDA graph capture. If set to 0 (default), enables GC freezing to speed up capture time. If set to 1, allows GC to run during capture.
# Possible values: "1" or "0"
VLLM_ENABLE_CUDAGRAPH_GC=0

# Used to force set up loopback IP.
# VLLM_LOOPBACK_IP=

# Used to set the process name prefix for vLLM processes. This is useful for debugging and monitoring purposes. The default value is "VLLM".
VLLM_PROCESS_NAME_PREFIX=VLLM

# Allow chunked local attention with hybrid kv cache manager. Currently using the Hybrid KV cache manager with chunked local attention in the Llama4 models (the only models currently using chunked local attn) causes a latency regression. For this reason, we disable it by default.
# Possible values: "1" or "0"
# VLLM_ALLOW_CHUNKED_LOCAL_ATTENTION_WITH_HYBRID_KV_CACHE=0