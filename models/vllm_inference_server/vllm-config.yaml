# vLLM configuration (reconstructed from original; changed values are uncommented)
# Source: original upload. :contentReference[oaicite:1]{index=1}

# ------------------------------------------ General Options ------------------------------------------
headless: false
api-server-count: 1
# disable-log-requests: true
disable-log-stats: false
# enable-prompt-adapter: false
no-enable-log-requests: false
# ------------------------------------------------------------------------------------------------------

# ---------------------------------- OpenAI-API Frontend Configuration ---------------------------------
host: 0.0.0.0
port: 8000
uvicorn-log-level: "info"
disable-uvicorn-access-log: false
allow-credentials: false
# allowed-origins:
# allowed-methods:
# allowed-headers:
# api-key: <removed - use env/secret>
# ------------------------------------------------------------------------------------------------------

# ---------------------------------- LoRA & Chat-template Configuration ---------------------------------
# lora-modules: None
# chat-template: None
chat-template-content-format: "auto"
response-role: "assistant"
# ssl-keyfile: None
# ssl-certfile: None
# ssl-ca-certs: None
enable-ssl-refresh: false
ssl-cert-reqs: 0
# root-path: None
# middleware: []

return-tokens-as-token-ids: false
disable-frontend-multiprocessing: false
enable-request-id-headers: false
enable-auto-tool-choice: false
exclude-tools-when-tool-choice-none: false
# tool-call-parser: None
# tool-parser-plugin: ""
# log-config-file: None
# max-log-len: None
disable-fastapi-docs: true
enable-prompt-tokens-details: false
enable-server-load-tracking: true
enable-force-include-usage: false
enable-tokenizer-info-endpoint: false
# ------------------------------------------------------------------------------------------------------

# --------------------------------------------- ModelConfig -----------------------------------------
# You can set HF-resolvable model path or a local path. This is a secure local model path (usig gguf format)
# Steps:
#  1. Download the model locally: `wget https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q4_K_M.gguf`
#  2. Mount the local model path to container's path
#  3. Set the path in the model key in the config file
model: Qwen/Qwen3-1.7B
# runner:
# convert:
# task: None
# tokenizer:
tokenizer-mode: "auto"
trust-remote-code: false
# dtype: "auto"
# seed: None
# hf-config-path: None
# allowed-local-media-path: ""
# revision: None
# code-revision: None
# tokenizer-revision: None
max-model-len: 3072
# quantization:
enforce-eager: false
max-seq-len-to-capture: 2048
max-logprobs: 20
logprobs-mode: "raw_logprobs"
# disable-sliding-window: false
# disable-cascade-attn: false
skip-tokenizer-init: false
enable-prompt-embeds: false
# served-model-name: None
disable-async-output-proc: false
config-format: "auto"
# hf-token: None
# hf-overrides: {}
# override-neuron-config: {}
# override-pooler-config: None
# logits-processor-pattern: None
generation-config: "auto"
# override-generation-config: {}
enable-sleep-mode: false
model-impl: "auto"
# override-attention-dtype: None
# ------------------------------------------------------------------------------------------------------

# --------------------------------------------- LoadConfig ---------------------------------------------
load-format: "auto"
# download-dir: None
# model-loader-extra-config: {}
# ignore-patterns: None
use-tqdm-on-load: true
# pt-load-map-location: "cpu"
# ------------------------------------------------------------------------------------------------------

# ------------------------------------------- DecodingConfig -------------------------------------------
guided-decoding-backend: "auto"
# guided-decoding-disable-fallback: false
# guided-decoding-disable-any-whitespace: false
# guided-decoding-disable-additional-properties: false
# reasoning-parser: ""
# ------------------------------------------------------------------------------------------------------

# ------------------------------------------- ParallelConfig -------------------------------------------
# distributed-executor-backend: None
# pipeline-parallel-size: 1
tensor-parallel-size: 1
# data-parallel-size: 1
# data-parallel-rank: None
# data-parallel-start-rank: None
# data-parallel-size-local: None
# data-parallel-address: None
# data-parallel-rpc-port: None
# data-parallel-backend: "mp"
# data-parallel-hybrid-lb: false
# enable-expert-parallel: false
# enable-eplb: false
# num-redundant-experts: 0
# eplb-window-size: 1000
# eplb-step-interval: 3000
# eplb-log-balancedness: false
# max-parallel-loading-workers: None
# ray-workers-use-nsight: false
# disable-custom-all-reduce: false
# worker-cls: "auto"
# worker-extension-cls: ""
# enable-multimodal-encoder-data-parallel: false
# ------------------------------------------------------------------------------------------------------

# --------------------------------------------- CacheConfig ---------------------------------------------
block-size: 32
gpu-memory-utilization: 0.95
swap-space: 8
kv-cache-dtype: "auto"
cpu-offload-gb: 16
# num-gpu-blocks-override: None
# enable-prefix-caching: None
# prefix-caching-hash-algo: "builtin"
# calculate-kv-scales: false
# kv-sharing-fast-prefill: false
# ------------------------------------------------------------------------------------------------------

# ------------------------------------------ MultiModalConfig -------------------------------------------
# limit-mm-per-prompt: {}
# media-io-kwargs: {}
# mm-processor-kwargs: None
# disable-mm-preprocessor-cache: false
# interleave-mm-strings: false
# ------------------------------------------------------------------------------------------------------

# --------------------------------------------- LoRAConfig ---------------------------------------------
# enable-lora: None
# enable-lora-bias: false
# max-loras: 1
# max-lora-rank: 16
# lora-extra-vocab-size: 256
# lora-dtype: "auto"
# max-cpu-loras: None
# fully-sharded-loras: false
# default-mm-loras: None
# ------------------------------------------------------------------------------------------------------

# ------------------------------------------ SpeculativeConfig -----------------------------------------
# speculative-config: None
# ------------------------------------------------------------------------------------------------------

# --------------------------------------- ObservabilityConfig ------------------------------------------
# show-hidden-metrics-for-version: None
# otlp-traces-endpoint: ""
# collect-detailed-traces: None
# ------------------------------------------------------------------------------------------------------

# ------------------------------------------- SchedulerConfig ------------------------------------------
max-num-batched-tokens: 4096
max-num-seqs: 5
# max-num-partial-prefills: 1
# max-long-partial-prefills: 1
# long-prefill-token-threshold: 0
# cuda-graph-sizes: []
# num-lookahead-slots: 0
# scheduler-delay-factor: 0.0
# preemption-mode: None
# num-scheduler-steps: 1
# multi-step-stream-outputs: true
# scheduling-policy: "fcfs"
# enable-chunked-prefill: true
# disable-chunked-mm-input: false
# scheduler-cls: "vllm.core.scheduler.Scheduler"
# disable-hybrid-kv-cache-manager: false
# async-scheduling: true
# compilation-config:
#   level: 3
#   use_inductor: true
#   use_cudagraph: true
#   CUDAgraph_capture_sizes: [128, 256]
# ------------------------------------------------------------------------------------------------------
