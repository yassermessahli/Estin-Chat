# Example .env file for Text Embeddings Inference Server configuration
# This file contains environment variables for configuring the Text-Embedding-Inference Server.
# NOTE: uncomment the variable only if you need to set it.

# Container specific configurations

# The name of the Docker container.
CONTAINER_NAME=
# The host port to map to the container's port.
HOST_PORT=
# The host path to mount as a volume for model data.
VOLUME_PATH=


# Text Embedding Webserver Configs

# The name of the model to load.
# Can be a MODEL_ID from hf.co/models (e.g., `BAAI/bge-large-en-v1.5`) or a local path.
MODEL_ID=

# The revision of the model on the hub.
# Can be a commit ID or a branch (e.g., `refs/pr/2`).
REVISION=

# The number of tokenizer workers.
# Defaults to the number of CPU cores.
TOKENIZATION_WORKERS=

# The dtype for the model.
# Possible values: `float16`, `float32`.
DTYPE=

# The pooling method for embedding models.
# If not set, it's parsed from the model's `1_Pooling/config.json`.
# If set, it overrides the model's pooling configuration.
# Possible values: `cls`, `mean`, `splade`, `last-token`.
POOLING=



# The maximum number of concurrent requests.
# Default: 512.
MAX_CONCURRENT_REQUESTS=

# The maximum total number of tokens in a batch.
# This is a critical control for hardware utilization.
# Default: 16384.
MAX_BATCH_TOKENS=

# The maximum number of individual requests in a batch.
MAX_BATCH_REQUESTS=

# The maximum number of inputs a client can send in a single request.
# Default: 32.
MAX_CLIENT_BATCH_SIZE=

# Automatically truncates inputs longer than the maximum supported size.
# Unused for gRPC servers.
AUTO_TRUNCATE=

# The name of the default prompt for encoding.
# Must be a key in the `sentence-transformers` configuration `prompts` dictionary.
# Cannot be used with `DEFAULT_PROMPT`.
DEFAULT_PROMPT_NAME=

# The default prompt text for encoding.
# The text will be prepended to the input.
# Cannot be used with `DEFAULT_PROMPT_NAME`.
DEFAULT_PROMPT=

# Your Hugging Face Hub token.
HF_TOKEN=

# The IP address to listen on.
# Default: 0.0.0.0.
HOSTNAME=

# The port to listen on.
# Default: 80.
PORT=80

# The Unix Domain Socket path for internal gRPC communication.
# Default: /tmp/text-embeddings-inference-server.
UDS_PATH=

# The location of the Hugging Face Hub cache.
# Default: /data.
HUGGINGFACE_HUB_CACHE=

# The payload size limit in bytes.
# Default: 2000000 (2MB).
PAYLOAD_LIMIT=

# The API key for request authorization.
# If set, requests must include an `Authorization: Bearer <API_KEY>` header.
API_KEY=

# Outputs logs in JSON format.
JSON_OUTPUT=

# Disables spans for telemetry.
DISABLE_SPANS=

# The OTLP/gRPC endpoint for telemetry.
# e.g., `http://localhost:4317`.
OTLP_ENDPOINT=

# The service name for OpenTelemetry.
# Default: text-embeddings-inference.server.
OTLP_SERVICE_NAME=

# The Prometheus port to listen on.
# Default: 9000.
PROMETHEUS_PORT=

# The CORS allowed origins for the HTTP server.
# Unused for gRPC servers.
CORS_ALLOW_ORIGIN=